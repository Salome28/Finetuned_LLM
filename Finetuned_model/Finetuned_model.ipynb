{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the dataset \n",
    "\n",
    "To train and test my model, I am using a dataset from **[Hugging face](https://huggingface.co/datasets/Hello-SimpleAI/HC3)**. The dataset consists of 6 subsets, each consisting of questions with answers from both humans and ChatGPT. These subsets are available for download as JSONL files, which can be seamlessly integrated into your code either by specifying their path or by directly loading them using the convenient **load_dataset function**.\n",
    "\n",
    "Choosing to use the JSONL files, I've decided to generate new JSON files containing solely the answers labeled 0 for human responses and 1 for ChatGPT responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"original_datasets/finance.jsonl\"\n",
    "output_file = \"answers.json\"\n",
    "\n",
    "# Function to extract human and ChatGPT answers from each line of JSONL\n",
    "def extract_data_from_line(line):\n",
    "    # Parse JSON from the line\n",
    "    json_data = json.loads(line)\n",
    "    # Extract human answers with label 0\n",
    "    human_answers = [{\"text\": answer, \"label\": 0} for answer in json_data[\"human_answers\"]]\n",
    "    # Extract ChatGPT answers with label 1\n",
    "    chatgpt_answers = [{\"text\": answer, \"label\": 1} for answer in json_data[\"chatgpt_answers\"]]\n",
    "    # Return both sets of answers\n",
    "    return human_answers, chatgpt_answers\n",
    "\n",
    "# Function to create a new JSON file with only the answers\n",
    "def create_new_json(input_file, output_file):\n",
    "    new_data = []  # List to hold all answers\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Extracting human and ChatGPT answers from each line\n",
    "            human_answers, chatgpt_answers = extract_data_from_line(line)\n",
    "            # Adding human answers to new_data\n",
    "            for answer in human_answers:\n",
    "                new_data.append(answer)\n",
    "            # Adding ChatGPT answers to new_data\n",
    "            for answer in chatgpt_answers:\n",
    "                new_data.append(answer)\n",
    "    \n",
    "    # Writing the new_data list to the output file in JSON format\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "# Calling create_new_json function with the input and output file paths\n",
    "create_new_json(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salomenkb/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bd5b511b3e46dc9fb284eb1f7180c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing the load_dataset function from the datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading dataset from JSON files into 'dataset' variable\n",
    "# 'train' and 'test' are the keys, and their corresponding values are the file paths\n",
    "dataset = load_dataset('json', data_files=('answers.json'))\n",
    "\n",
    "# Splitting the dataset into train and test sets\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, shuffle=True)\n",
    "\n",
    "# Importing the AutoTokenizer class from the transformers library\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiating an AutoTokenizer object with a pre-trained DistilBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 6748\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1688\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0bff9ac94247ba83137b430749f94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59231cdade9043599fe9f7994f6dcbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1688 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a preprocess_function \n",
    "def preprocess_function(data):\n",
    "    # Tokenize the text using the tokenizer\n",
    "    # \"text\" is the key containing the text data in each example\n",
    "    return tokenizer(data[\"text\"], truncation= True, max_length= 800)\n",
    "\n",
    "# Apply the preprocess_function to the dataset using the map() method\n",
    "# batched as True enables mapping in batches for better performance\n",
    "# This function will tokenize the text data in the \"text\" field of each element in the dataset\n",
    "tokenized_answers = split_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the DataCollatorWithPadding class from the transformers library\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Creating a DataCollatorWithPadding object\n",
    "# This class handles padding of sequences to ensure they all have the same length\n",
    "# It takes the tokenizer object as an argument, which is used for padding sequences\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the evaluate module\n",
    "import evaluate\n",
    "\n",
    "# Loading the accuracy evaluation metric using the load function from the evaluate module\n",
    "accuracy = evaluate.load(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the numpy library as np\n",
    "import numpy as np\n",
    "\n",
    "# Defining a function compute_metrics \n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpacking the eval_pred tuple into predictions and labels\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Computing the index of the maximum value along axis 1 in the predictions array\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Returning the accuracy score based on the predicted labels and true labels\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping from labels to corresponding class names\n",
    "id2label = {0: \"HUMAN\", 1: \"CHATGPT\"}\n",
    "\n",
    "# Dictionary mapping from class names to corresponding labels\n",
    "label2id = {\"HUMAN\": 0, \"CHATGPT\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary classes from the transformers library\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Instantiating a model for sequence classification using AutoModelForSequenceClassification loaded from the pre-trained DistilBERT model \n",
    "# num_labels specifies the number of labels for classification (Human and Chatgpt)\n",
    "# id2label and label2id are dictionaries mapping label indices to label names and vice versa\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing classes from the transformers library\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Defining training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"My_first_model\",  # Directory where the model checkpoints and evaluation results will be saved\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=16,  # Batch size for training per device\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation per device\n",
    "    num_train_epochs=2,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay to apply during optimization\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation strategy (evaluated after each epoch)\n",
    "    save_strategy=\"epoch\",  # Model saving strategy (save checkpoint after each epoch)\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer class\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_answers[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_answers[\"test\"],  # Evaluation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer used for tokenizing input sequences\n",
    "    data_collator=data_collator,  # Data collator for batch processing\n",
    "    compute_metrics=compute_metrics,  # Function for computing evaluation metrics\n",
    ")\n",
    "\n",
    "# Start training the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
